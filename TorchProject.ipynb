{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL8BJ//EoXtelTKyhgrXSj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dosubeen630/AI-project/blob/main/%08TorchProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XXt1XHSMIPF"
      },
      "outputs": [],
      "source": [
        "# Minist data 준비\n",
        "from pathlib import Path\n",
        "import requests\n",
        "# 경로설정을 담당하는 python3 라이브러리의 일부인 pathlib, 데이터 셋을 다운로드할 requests 모듈을 불러온다.\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "        content = requests.get(URL + FILENAME).content\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)\n",
        "        # 위의 if문은 위의 url에서  filename이 지정된 경로에 없으면 다운 받아라 라는 의미이다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "               ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f , encoding=\"latin-1\")\n",
        "\n",
        "# 이 데이터 셋은 Numpy 배열 포맷이고, 데이터 직렬화하기 위한 파이썬 전용 포맷 pickle을 이용하여 저장되어 있음\n"
      ],
      "metadata": {
        "id": "0vmuzjhrbtz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "# 그래프를 그리기 위한 맷플로립과 넘파이 모듈을 불러온다.\n",
        "pyplot.imshow(x_train[0].reshape((28,28)), cmap=\"gray\")\n",
        "print(x_train.shape) # 첫번째 x_train 의 이미지를  화면에 출력해본다.\n",
        "# MNIST은 손글씨 이미지 데이터 집합으로 이미지는 그레이스케일 이미지이며, 28*28 픽셀크기에 숫자 0부터 9중 하나가 씌여있다.\n",
        "# 데이터는 일반적인 이미지 포맷이 아니고 각 픽셀의 강도와 숫자 레이블의 쌍이다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "gSm2H7FzdYRv",
        "outputId": "b7afe3fb-acf6-410e-edaa-7b7b64e2dad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch는 NumPy 배열보다는  torch.tensor를 사용하므로 데이터 변환을 해야함. 아래의 코드는 데이터 변환을 하는 것임\n",
        "import torch\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
        "n,c = x_train.shape\n",
        "print(x_train,y_train)\n",
        "print(x_train.shape)\n",
        "print(y_train.min(), y_train.max())\n",
        "# 위에서 다운 받은 이미지의 넘파이 배열을 텐서로 변환해주는 것임. 그리고 나서 변환된 데이터를 확인 하기 위해 첫번째 출력해봄. 두번째는 의 사이즈 확인, 세번째는 의 최소값과 최대값을 출력을 통해 알 수 있다.\n",
        "#x_train은 훈련 데이터 의미 y_train 훈련 데이터의 레이블을 의미"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukCnLogOeaau",
        "outputId": "775c830e-b6d0-4629-a4bb-2c30b3c58a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4c5525f8577b>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
            "torch.Size([50000, 784])\n",
            "tensor(0) tensor(9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"torch.nn\" 없이 밑바닥부터 신경망 만들기\n",
        "import math\n",
        "\n",
        "weights = torch.randn(784,10) / math.sqrt(784)\n",
        "print(weights)\n",
        "weights.requires_grad_()\n",
        "print(weights.requires_grad_())\n",
        "bias = torch.zeros(10, requires_grad=True)\n",
        "print(bias)\n",
        "# 간단한 선형 모델의 가중치(weights)와 절편(bias)을 생성 . requires_grad를 통해 가중치를 초기화 . 절편도 초기화 시켜줌."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95WpbYqPkFmx",
        "outputId": "6e06d128-3889-42ff-cdc2-17e6a08bf34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0129,  0.0320, -0.0633,  ...,  0.0013, -0.0486,  0.0421],\n",
            "        [ 0.0067,  0.0741,  0.0642,  ...,  0.0218, -0.0020, -0.0202],\n",
            "        [-0.0469,  0.0158, -0.0483,  ..., -0.0430,  0.0077,  0.0274],\n",
            "        ...,\n",
            "        [ 0.0212,  0.0014,  0.0571,  ..., -0.0045,  0.0451, -0.0068],\n",
            "        [-0.0266, -0.0205,  0.0284,  ...,  0.0564, -0.0719, -0.0283],\n",
            "        [ 0.0640,  0.0658,  0.0186,  ...,  0.0349, -0.0172, -0.0162]])\n",
            "tensor([[-0.0129,  0.0320, -0.0633,  ...,  0.0013, -0.0486,  0.0421],\n",
            "        [ 0.0067,  0.0741,  0.0642,  ...,  0.0218, -0.0020, -0.0202],\n",
            "        [-0.0469,  0.0158, -0.0483,  ..., -0.0430,  0.0077,  0.0274],\n",
            "        ...,\n",
            "        [ 0.0212,  0.0014,  0.0571,  ..., -0.0045,  0.0451, -0.0068],\n",
            "        [-0.0266, -0.0205,  0.0284,  ...,  0.0564, -0.0719, -0.0283],\n",
            "        [ 0.0640,  0.0658,  0.0186,  ...,  0.0349, -0.0172, -0.0162]],\n",
            "       requires_grad=True)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 로그 소프트 맥스 함수와 모델 함수 정의\n",
        "def log_softmax(x):\n",
        "  return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "\n",
        "print(log_softmax)\n",
        "def model(xb):\n",
        "  return log_softmax(xb @ weights + bias)  #@ 기호는 행렬곱셈 연산을 나타냄\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnxQcCm-oqbK",
        "outputId": "988824a9-2c56-4756-867a-8c2b087f72ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function log_softmax at 0x7ef442b96290>\n",
            "<function model at 0x7ef514c9d000>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 64  #배치 크기\n",
        "\n",
        "xb = x_train[0:bs]  # x 로부터 미니배치 (mini-batch) 추출\n",
        "preds = model(xb)   # 예측\n",
        "preds[0], preds.shape\n",
        "print(preds[0], preds.shape) #예측 값 출력, preds 텐서는 텐서값 이외에도, 또한 기울기 함수(gradient function)를 담고 있음."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP1iVtRJrc_S",
        "outputId": "2a6fb569-1929-4601-e6b0-337f6868bc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.9297, -2.2512, -1.7928, -2.4149, -2.1338, -2.6799, -2.7325, -2.2151,\n",
            "        -2.9687, -2.5117], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실함수로 사용하기 위한 음의 로그 우도(negative log-likelihood) 구현\n",
        "\n",
        "def null(input, target):\n",
        "  return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = null\n",
        "print(loss_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocYoagDRtkZO",
        "outputId": "517d5888-2d48-4897-aa24-3f547e08acb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function null at 0x7ef442b36560>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에 만든 무작위 모델에 대한 손실을 점검\n",
        "yb = y_train[0:bs]\n",
        "print(loss_func(preds,yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEKed-UKvALy",
        "outputId": "8f9d35f6-065a-426c-9b9e-d27281581df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3432, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에 만든 모델의 정확도(accuracy)를 계산하기 위한 함수 구현 (만약 가장 큰 값의 인덱스가 목표값(target value)과 동일하면, 그 예측은 올바른 것이라고 판단)\n",
        "\n",
        "def accuracy(out, yb):\n",
        "  preds = torch.argmax(out, dim=1)\n",
        "  return (preds == yb).float().mean()\n",
        "\n",
        "\n",
        "print(accuracy(preds, yb)) # 모델 정확도 계산 함수 호출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jZygLZlvAM0",
        "outputId": "bd8fab3a-4a96-4100-de94-27b572a92bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 루프 만들기\n",
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "import torch\n",
        "import pickle\n",
        "import gzip\n",
        "import requests\n",
        "import math\n",
        "\n",
        "weights = torch.randn(784,10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)\n",
        "\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "        content = requests.get(URL + FILENAME).content\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "               ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f , encoding=\"latin-1\")\n",
        "\n",
        "lr = 0.5   # 학습률\n",
        "epochs = 2  # 훈련에 사용할 에폭 수\n",
        "x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
        "n,c = x_train.shape\n",
        "\n",
        "def model(xb):\n",
        "  return xb@ weights + bias  #matrix multipication\n",
        "\n",
        "def accuracy(out, yb):\n",
        "  preds = torch.argmax(out, dim=1)\n",
        "  return (preds == yb).float().mean()\n",
        "\n",
        "bs = 64 #배치크기\n",
        "for epochs in range(epochs):\n",
        "    for i in range((n-1)// bs +1):\n",
        "                 # set_trace()\n",
        "        start_i = i * bs\n",
        "        end_i = start_i + bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)   # 모델을 이용하여 예측 수행\n",
        "        loss = loss_func(pred, yb)  #손실 계산\n",
        "\n",
        "        loss.backward()  # 모델의 기울기 업데이트\n",
        "        with torch.no_grad():\n",
        "          weights -= weights.grad * lr\n",
        "          bias -= bias.grad * lr\n",
        "          weights.grad.zero_()\n",
        "          bias.grad.zero_()\n",
        "          # 이름 별로 각 매개변수(파라미터)의 값을 업데이트 하고 각 매개변수에 대한 기울기들을 개별적으로 수동으로 0으로 제거 함\n",
        "\n",
        "print(loss_func(model(xb),yb), accuracy(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8D5UTkGw-1Y",
        "outputId": "dc1b1cb4-51ef-4ea4-b5ee-ef482c205d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0829, grad_fn=<NllLossBackward0>) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.nn.functional 사용하기\n",
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.cross_entropy     # 음의 로그 우도 손실과 로그 소프트맥스 활성화 함수를 사용하는 경우 파이토치는 이 둘을 결합하는 단일 함수인 F.cross_entropy 제공. 활성화함수를 제거 가능\n",
        "\n",
        "def model(xb):\n",
        "  return xb @ weights + bias     # 위의 모델 함수의 경우는 로그 소프트 맥스 활성화 함수를 호출하는데  이 경우에는 로그 소프트 맥스 함수를 호출 하지 않고 있습니다."
      ],
      "metadata": {
        "id": "DV2zMbMH0rm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5hPdA292LNs",
        "outputId": "72b9e505-7b0c-4d0a-df87-14651e26ad9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0672, grad_fn=<NegBackward0>) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Module을 이용하여 리팩토링 하기\n",
        "from torch import nn\n",
        "\n",
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(784,10)/ math.sqrt(784))\n",
        "        self.bias = nn.Parameter(torch.zeros(10))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return xb @ self. weights + self.bias\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Mnist_Logistic()  # 모델을 인스턴스화\n",
        "\n",
        "print(loss_func(model(xb),yb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "357NFwnu2vuW",
        "outputId": "234043f5-9260-4b97-8f30-279e2ef619d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.1654, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 루프의 간소화, 함수로 작은 훈련루프 감싸기\n",
        "\n",
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range((n-1)// bs +1):\n",
        "            start_i = i * bs\n",
        "            end_i = start_i + bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred,yb)\n",
        "\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters():\n",
        "                    p -= p.grad * lr\n",
        "                model.zero_grad()\n",
        "\n",
        "fit()\n",
        "\n",
        "print(loss_func(model(xb), yb)) # 손실이 줄었는지 확인 하기 위해 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsYIJNzU6iNH",
        "outputId": "223112e7-d1c4-4921-a364-f5eb2fac0503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-3578.6531, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Linear 를 사용하여 리팩토링 하기\n",
        "# (self.wights 및 self.bias를 수동으로 정의 및 초기화하고\n",
        "# xb@ self.weights + self.bias를 계산하는 대신에 파이토치의 클래스인 nn.Linear 선형레이어로 사용)\n",
        "\n",
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(784,10)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.lin(xb)\n",
        "# 이전과 같은 방식으로 모델을 인스턴스화하고 손실을 계산합니다.\n",
        "model = Mnist_Logistic()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3G4Vhlb8l1o",
        "outputId": "d9f4c532-098c-4d71-df04-4e19c39cce9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0321, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전과 동일한 fit 메소드를 사용하여 출력\n",
        "\n",
        "fit()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi6HTlB7-cUK",
        "outputId": "248cf30d-aecb-447c-bca0-e9a1be6f8ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-1789.2119, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.optim 을 이용하여 리팩토링 하기\n",
        "# 각 매개변수를 수동으로 업데이트 하는 대신에 옵티마이져의 step 메소드를 사용하여 업데이트 진행가능\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "def get_model():\n",
        "    model = Mnist_Logistic()\n",
        "    return model, optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "model, opt = get_model()\n",
        "print(loss_func(model(xb), yb))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n-1) // bs+1):\n",
        "        start_i = i * bs\n",
        "        end_i = start_i + bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad() # 기울기를 0으로 재설정 해줍니다. 다음 미니 배치에 대한 기울기를 계산하기 전에 호출 해야함.\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5YBpUZY-356",
        "outputId": "46cf84ba-5f47-4268-ed6a-9ca0dc6b5d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1419, grad_fn=<NegBackward0>)\n",
            "tensor(-1789.1019, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 을 이용하여 리팩토링하기\n",
        "# Pytorch의 TensorDataset은 텐서를 감싸는 (wrapping) Dataset 입니다.\n",
        "# 길이와 인덱싱 방식을 정의함으로서 텐서의 첫번째 차원을 따라 반복, 인덱싱 및 슬라이스 하는 방법도 제공합니다. 동일한 라인에서의 훈련시 독립변수와 종속변수에 쉽게 엑세스 가능\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "model, opt = get_model()\n",
        "train_ds = TensorDataset(x_train, y_train)  # x_train, y_train 모두 하나의 TensorDataset 에 합쳐 질수 있음. 반복시키고 슬라이스 하기 편리해짐\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n-1) // bs +1):\n",
        "        xb, yb = train_ds[i*bs: i*bs + bs] # 이전에는 x및 y값의 미니배치를 별도로 반복해야 했으나, 이제 두 단계를 함께 수행가능.\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "insm4naaR_Ru",
        "outputId": "bd57a5f9-b526-4041-901e-f7198f51331b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-1789.2233, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader 를 사용하여 리팩토링 하기\n",
        "# PyTorch의 DataLoard 는 배치 관리 담당. 배치들에 대해서 반복하기 쉽게 만들어줌.\n",
        "#  train_ds[i*bs : i*bs+bs] 를 사용하는 대신에 DataLoard는 미니배치를 자동적으로 제공함.\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs)\n",
        "\n",
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in train_dl:\n",
        "      pred = model(xb)\n",
        "      loss = loss_func(pred, yb)   # (xb,yb)가 DataLoard 에서 자동으로 로드되므로 루프가 깔끔해짐. 이전 루프에서는 배치(xb,yb)를 반복했음.\n",
        "\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb),yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmekYWZWWY2V",
        "outputId": "7c016205-72ea-4ce4-da26-5cb8935418de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1089, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증(validation) 추가하기\n",
        "# overfitting을 확인 하기 위해서는 항상 검증데이터셋(vaildation set)이 있어야함.\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True) # 각 에포크마다 데이터를 셔플링할지 여부 기본값(false)\n",
        "\n",
        "valid_ds = TensorDataset(x_valid,y_valid)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs*2)\n",
        "\n",
        "# 검증 데이터셋에 대한 배치 크기는 락습 데이터셋 배치 크기의 2배를 사용. 이는 검증 데이터 셋에 대해서는 역전파가 필요하지 않기 때문\n",
        "# 더 큰 배치 크기를 사용하여 손실을 더 빨리 계산하기 위함.\n",
        "\n",
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # 훈련 전에 항상 호출\n",
        "    for xb, yb in train_dl:\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    model.eval() # 추론 전에 호출\n",
        "    with torch. no_grad():\n",
        "        valid_loss = sum(loss_func(model(xb),yb) for xb, yb in valid_dl)\n",
        "\n",
        "    print(epoch, valid_loss / len(valid_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOhf6faLUbmo",
        "outputId": "9fbab0f9-9086-4cca-9314-9e4ec00df784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.2977)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  fit()와 get_data()생성하기\n",
        "# 훈련 데이터셋과 검증 데이터셋 모두에 대한 손실을 계산하는 유사한 프로세스를 두번 거치는 것을\n",
        "# 하나의 배치에 대한 손실을 계산하는 자체 함수 loss_batch 만들기\n",
        "\n",
        "def loss_batch(model, loss_func, xb,yb, opt = None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)\n",
        "\n",
        "# 훈련 데이터셋에 대한 옵티마이저를 전달하고 이를 사용하여 역전파를 수행합니다.\n",
        "# 검증데이터셋의 경우 옵티마이저를 전달 하지 않으므로 메소드가 역전파를 수행하지 않습니다."
      ],
      "metadata": {
        "id": "t4svy6tPchXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit 은 모델을 훈련하고 각 에폭에 대한 훈련 및 검증 손실을 계산하는 작업을 수행\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb,yb in train_dl:\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, nums = zip(\n",
        "                *[loss_batch(model, loss_func,xb,yb)for xb,yb in valid_dl]\n",
        "            )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "        print(epoch, val_loss)"
      ],
      "metadata": {
        "id": "SCbAHt8kgYWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_data는 학습 및 검증 데이터셋에 대한 dataloader를 출력합니다.\n",
        "\n",
        "def get_data(train_ds, valid_ds, bs):\n",
        "    return(\n",
        "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "        DataLoader(valid_ds, batch_size=bs*2)\n",
        "    )\n",
        "def loss_batch(model, loss_func, xb,yb, opt = None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True) # 각 에포크마다 데이터를 셔플링할지 여부 기본값(false)\n",
        "\n",
        "valid_ds = TensorDataset(x_valid,y_valid)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs*2)\n",
        "# dataloader을 가져오고 모델을 훈련하는 전체 프로세스\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "model, opt = get_model()\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n"
      ],
      "metadata": {
        "id": "ICadvG6XiBHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN으로 넘어가기\n",
        "# 3개의 컨볼루션 레이어로 신경망을 구축. 파이토치에 사전정의된 Conv2d 클래스를 컨볼루션 레이어로 사용.\n",
        "# 3개의 컨볼루션 레이어로 CNN을 정의. 각 컨볼루션 뒤에는 렐루가 있습니다. 마지막으로 평균 풀링을 수행함.\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class Mnist_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.view(-1,1,28,28)\n",
        "        xb = F.relu(self.conv1(xb))\n",
        "        xb = F.relu(self.conv2(xb))\n",
        "        xb = F.relu(self.conv3(xb))\n",
        "        xb = F.avg_pool2d(xb,4)\n",
        "        return xb.view(-1, xb.size(1))  #  view의 경우 파이토치의 넘파이 reshape 버전입니다.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb,yb in train_dl:\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          losses, nums = zip(\n",
        "              *[loss_batch(model, loss_func,xb,yb)for xb,yb in valid_dl]\n",
        "          )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "        print(epoch, val_loss)\n",
        "lr = 0.1\n",
        "\n",
        "# Momentum은 이전 업데이트도 고려하고 일반적으로 더 빠른 훈련으로 이어지는 확률적 경사하강법의 변형임.\n",
        "model = Mnist_CNN()\n",
        "opt = optim.SGD(model.parameters(),lr=lr, momentum=0.9)\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "fit(5, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ-nvBOPkWXm",
        "outputId": "7e749986-c1a3-410c-a03c-4d1d42917160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.30486197896003725\n",
            "1 0.235096559458971\n",
            "2 0.20248531655073165\n",
            "3 0.17488175463676453\n",
            "4 0.16038697510659694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn. Sequential 사용하기\n",
        "#  Sequential 객체는 그 안에 포함된 각 모듈을 순차적으로 실행 함. 우리의 신경망을 작성하는 간단한 방법\n",
        "# 이를 활용하려면 주어진 함수에서 사용자 정의 레이어(custom layer)를 쉽게 정의 할 수 있어야합니다.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(784,10)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.lin(xb)\n",
        "def get_model():\n",
        "    model = Mnist_Logistic()\n",
        "    return model, optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
        "n,c = x_train.shape\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True) # 각 에포크마다 데이터를 셔플링할지 여부 기본값(false)\n",
        "\n",
        "valid_ds = TensorDataset(x_valid,y_valid)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs*2)\n",
        "\n",
        "# 검증 데이터셋에 대한 배치 크기는 락습 데이터셋 배치 크기의 2배를 사용. 이는 검증 데이터 셋에 대해서는 역전파가 필요하지 않기 때문\n",
        "# 더 큰 배치 크기를 사용하여 손실을 더 빨리 계산하기 위함.\n",
        "model, opt = get_model()\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "\n",
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "def preprocess(x):\n",
        "    return x.view(-1,1,28,28)\n",
        "\n",
        "\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb,yb in train_dl:\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          losses, nums = zip(\n",
        "              *[loss_batch(model, loss_func,xb,yb)for xb,yb in valid_dl]\n",
        "          )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "        print(epoch, val_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sequential로 생성된 모듈\n",
        "model = nn.Sequential(\n",
        "    Lambda(preprocess),\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride = 2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride = 2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride = 2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(4),\n",
        "    Lambda(lambda x: x.view(x.size(0),-1)),\n",
        ")\n",
        "lr = 0.5 #학습률\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "epochs = 2 #훈련에 사용할 에폭 수\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI9Gg8y7oCGM",
        "outputId": "0cd56d37-065f-45b2-92a2-d8c4a89a4f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-b726dd667f0f>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader 감싸기\n",
        "def preprocess(x, y):\n",
        "    return x.view(-1, 1, 28, 28), y\n",
        "\n",
        "\n",
        "class WrappedDataLoader:\n",
        "    def __init__(self, dl, func):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b))\n",
        "\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n",
        "\n",
        "# 초기의 lambda 레이어를 제거하고 데이터 전처리를 제너레이터로 이동\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "# nn.AvgPool2d 를 nn.AdaptiveAvgPool2d 로 대체하여 우리가 가진 입력텐서가 아닌 원하는 출력텐서의 크기를 정의\n",
        "# 결과적으로 우리 모델은 모든 크기의 입력과 함께 작동\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYwHTymvQqoB",
        "outputId": "f62e89d0-f704-401a-f825-5f1159bcc511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.2738343547821045\n",
            "1 1.2258491744995117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #GPU 사용하기\n",
        "\n",
        " dev = torch.device(\n",
        "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    # 디바이스 오브젝트 생성\n",
        "\n",
        "def preprocess(x, y):\n",
        "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
        "\n",
        "\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n",
        "\n",
        "#GPU로 배치를 옮기도록 preprocess 업데이트\n",
        "\n",
        "model.to(dev)\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "# 모델을 GPU로 이동"
      ],
      "metadata": {
        "id": "t1trxO6kSLTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> MNIST 튜토리얼을 따라 해보며 파이토치의 모듈을 사용해 보고 편하였던 점과 각 모듈 정의<h1>\n",
        "  \n",
        "  1. **nn.Module을 이용하여 리팩토링하기**\n",
        "***\n",
        " * 이것을 사용하여 사용하고자 하는 몇가지 중요한 속성이나 메소드들을 가지고 있습니다. 예를들어  훈련루프를 위해 이름별로 각 매개변수(파라미터)의 값을 업데이트 하고 각 매개변수에 대한 기울기를 개별적으로 수동으로 0으로 제거 해야했습니다.\n",
        " model.parameters() 나 model.zero_grad()를 활용하여 이러한 단계를 간결하게 만들어 주고, 복잡한 모델에서 일부 매개변수를 잊어 버리는 오류를 덜 발생 시킬수 있습니다.\n",
        "<br> nn.Module에는 미리 정의 되어 있는 Layer(모델) 계층이 있어서 보다 사용하기에 편리합니다. 위에 프로젝트에서는 레이어 중에 nn.Linear를 사용하여 중요한 변수인 가중치나 편향 등의 변수를 기억하고\n",
        " 선형레이어로 만들어 주어 기존의 코드보다 속도를 빠르게 만들어 줍니다.\n",
        "~~~pyhton\n",
        "    with torch.no_grad():\n",
        "    weights -= weights.grad * lr\n",
        "    bias -= bias.grad * lr\n",
        "    weights.grad.zero_()\n",
        "    bias.grad.zero_()\n",
        "~~~\n",
        "~~~python\n",
        "    with torch.no_grad():\n",
        "    for p in model.parameters(): p -= p.grad * lr\n",
        "    model.zero_grad()\n",
        "~~~\n",
        "\n",
        "  2. **torch.nn.functional 사용하여 리팩토링 하기**\n",
        "***\n",
        "<br><변경 전>\n",
        "~~~python\n",
        "  def log_softmax(x):\n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "\n",
        "def model(xb):\n",
        "    return log_softmax(xb @ weights + bias)\n",
        "~~~    \n",
        "<br><변경 후>\n",
        "~~~python\n",
        "  import torch.nn.functional as F\n",
        "\n",
        "  loss_func = F.cross_entropy\n",
        "\n",
        "  def model(xb):\n",
        "      return xb @ weights + bias\n",
        "~~~    \n",
        "\n",
        "<br>위에서 직접 작성한 코드를 짧게 만들고 사용하기에 편리하게 만들려면 가장 먼저 직접 작성한 활성화 함수, 손실 함수를 nn.functional의 함수로 대체 하는 것입니다.\n",
        "<br>우리의 코드 중에서는 음의 로그우도 손실과 소프트 맥스 함수를 파이토치 에서는 이 둘을 결합하는 단일 함수인 F.cross_entropy를 제공합니다.\n",
        "\n",
        "***\n",
        "  \n",
        "  3. **torch.optim 을 이용하여 리팩토링하기**\n",
        "<br>파이토치 에서는 다양한 최적화 알고리즘을 가진 패키지로 torh.optim 을 제공하고 있는데 이는 각 매개변수를 수동으로 업데이트 하는 대신, 옵티마이져의 step 메소드를 사용하여 업데이트를 진행 할 수 있습니다.\n",
        "<br> 학습 시에는 다음과 같은 하이퍼파라미터를 정의합니다:\n",
        "<br> **에폭(epoch) 수** - 데이터셋을 반복하는 횟수\n",
        "<br> **배치 크기(batch size)** - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n",
        "<br> **학습률(learning rate)** - 각 배치/에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있습니다.\n",
        "<br> 또한 optim.sero_grad()는 기울기를 0으로 재설정 해줍니다.\n",
        "<br> 하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있습니다. 최적화 단계의 각 반복(iteration)을 에폭이라고 부릅니다.\n",
        "<br> 하나의 에폭은 다음 두 부분으로 구성됩니다:\n",
        "<br> **학습 단계(train loop)** - 학습용 데이터셋을 반복(iterate)하고 최적의 매개변수로 수렴합니다.\n",
        "<br> **검증/테스트 단계(validation/test loop)** - 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복(iterate)합니다.\n",
        "<br> 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정입니다.\n",
        "~~~python\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "~~~\n",
        "<br> 최적화 알고리즘은 이 과정이 수행되는 방식(여기에서는 확률적 경사하강법(SGD; Stochastic Gradient Descent))을 정의합니다.\n",
        "<br> 모든 최적화 절차(logic)는 optimizer 객체에 캡슐화(encapsulate)됩니다. 여기서는 SGD 옵티마이저를 사용하고 있다.\n",
        "<br> 학습 단계(loop)에서 최적화는 세단계로 이뤄집니다:\n",
        "<br> optimizer.zero_grad()를 호출하여 모델 매개변수의 변화도를 재설정합니다. 기본적으로 변화도는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정합니다.\n",
        "<br> loss.backwards()를 호출하여 예측 손실(prediction loss)을 역전파합니다. PyTorch는 각 매개변수에 대한 손실의 변화도를 저장합니다.\n",
        "<br> 변화도를 계산한 뒤에는 optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정합니다.\n",
        "***\n",
        "  \n",
        "  4. **Dataset을 이용하여 리팩토링 하기**\n",
        "\n",
        "  ~~~python\n",
        "  from torch.utils.data import TensorDataset\n",
        "  ~~~\n",
        "  <br> <변경 전>\n",
        "  ~~~python\n",
        "  xb = x_train[start_i:end_i]\n",
        "  yb = y_train[start_i:end_i]\n",
        "  ~~~\n",
        "  <br> <변경 후>\n",
        "  ~~~python\n",
        "  train_ds = TensorDataset(x_train, y_train)\n",
        "  xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "  ~~~\n",
        "<br> 파이토치의 TensorDataset은 텐서를 감싸는 데이터 셋 입니다.\n",
        "<br> 길이와 인덱싱의 방식을 정의 함으로써 텐서의 첫번째 차원을 따라 반복, 인덱싱 및 슬라이스 하는 방법도 제공합니다. 이렇게 되면 훈련시에 동일한 라인에서 독립 변수와 종속 변수에 쉽게 접근 할 수 있습니다.\n",
        "<br> 데이터 로딩과 전처리를 담당하는 Dataset 클래스는 모델 학습에 필수적인 기능을 제공합니다.\n",
        "<br> Dataset 클래스는 사용자가 정의하는 데이터셋에 대한 인터페이스를 제공합니다. 사용자는 Dataset 클래스를 상속받아 자신의 데이터셋에 맞게 커스텀 데이터셋 클래스를 구현합니다.\n",
        "<br> Dataset 클래스는 데이터셋의 샘플을 가져오고, 전처리를 수행하며, 샘플의 개수를 반환하는 등의 기능을 제공합니다.\n",
        "<br> Dataset 클래스를 사용하면 데이터셋을 효율적으로 로딩하고 전처리할 수 있으며, 모델 학습에 필요한 데이터를 제공할 수 있습니다.\n",
        "<br> 또한, DataLoader 클래스와 함께 사용하여 데이터의 배치(batch) 처리와 데이터 로딩의 병렬 처리 등을 간편하게 구현할 수 있습니다.\n",
        "<br> 사용자가 직접 커스텀 데이터셋 클래스를 구현하면, 데이터셋을 더욱 유연하게 처리할 수 있고, 모델 학습에 필요한 데이터를 원하는 형태로 로딩하고 전처리할 수 있습니다.\n",
        "***\n",
        "  \n",
        "  5. **DataLoard를 사용하여 리팩토링 하기**\n",
        "  <br>\n",
        "  ~~~python\n",
        "  from torch.utils.data import DataLoader\n",
        "  train_ds = TensorDataset(x_train, y_train)\n",
        "  train_dl = DataLoader(train_ds, batch_size=bs)\n",
        "  ~~~\n",
        "<br> 파이토치에서 데이터로더는 배치관리를 담당 합니다. 모든 데이터셋 으로부터 데이터 로더를 생성 할 수 있습니다. 또한, 배치 들에 대해서 반복하기 쉽게 만들어 줍니다.\n",
        "<br> 매번 미니 배치를 자동적으로 제공 하여 줍니다.\n",
        " <br>  <변경 전>\n",
        " ~~~python\n",
        " for i in range((n-1)//bs + 1):\n",
        "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "    pred = model(xb)\n",
        "    ~~~\n",
        " <br> <변경 후>\n",
        " ~~~python\n",
        "  for xb,yb in train_dl:\n",
        "    pred = model(xb)\n",
        "    ~~~   \n",
        "<br> DataLoader는 파이토치(PyTorch)에서 제공하는 데이터 로딩 유틸리티로, 모델 학습 시에 데이터를 배치(batch) 단위로 로드하여 효율적인 학습을 가능하게 해주는 클래스입니다.\n",
        "<br> DataLoader를 사용하면 인자들을 조정하여 데이터 로딩의 성능을 최적화할 수 있습니다. 또한, DataLoader는 반복 가능한(iterable) 객체로, for 루프를 사용하여 데이터를 배치 단위로 간단하게 로드할 수 있습니다. 배치 단위로 로드된 데이터는 모델에 전달되어 학습이 이루어집니다.\n",
        "  ***\n",
        "  \n",
        "  6. **Conv2d 클래스, 모멘텀**\n",
        "  <br> nn.Conv2d는 파이토치에서 제공하는 2D 컨볼루션 레이어 클래스입니다. 이미지나, 2D 데이터 특징 추출에 주로 사용되며, 합성곱 신경망(CNN)에서 핵심적인 레이어 중 하나 입니다.\n",
        "  <br> nn.Conv2d는 다음과 같이 사용 됩니다.\n",
        "  \n",
        "<br> `nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)`\n",
        "\n",
        "  <br> **in_channels (int)**: 입력 채널의 개수. 예를 들어, RGB 이미지의 경우 3개의 채널을 가지므로 in_channels는 3이 됩니다.\n",
        "  <br> **out_channels (int)**: 출력 채널의 개수, 즉 컨볼루션 필터의 개수입니다. 이 값이 클수록 더 복잡한 특징을 학습할 수 있지만, 모델의 파라미터 수가 증가하게 됩니다.\n",
        "  <br> **kernel_size (int 또는 tuple)**: 컨볼루션 필터의 크기. 예를 들어, 3x3 필터의 경우 kernel_size는 3 또는 (3, 3)으로 지정할 수 있습니다.\n",
        "  <br> **stride (int 또는 tuple, optional)**: 필터의 이동 간격, 즉 스트라이드(stride)입니다. 기본값은 1이며, 더 큰 값으로 설정하면 출력 특징 맵의 크기가 작아지게 됩니다.\n",
        "  <br> **padding (int 또는 tuple, optional)**: 입력 데이터의 가장자리에 추가되는 패딩의 크기입니다. 기본값은 0이며, 패딩을 사용하면 출력 특징 맵의 크기를 보존하면서 입력 데이터의 가장자리 정보를 유지할 수 있습니다.\n",
        "  <br> **dilation (int 또는 tuple, optional)**: 딜레이션(dilation) 레이트입니다. 딜레이션은 필터의 간격을 더 크게 두어 더 넓은 영역의 정보를 가져오는 데 사용됩니다. 기본값은 1이며, 값이 커질수록 필터의 영역이 더 넓어지게 됩니다.\n",
        "  <br> **groups (int, optional)**: 입력 및 출력 채널을 묶는(grouping) 개수입니다. 기본값은 1이며, 값이 크면 채널 간의 관련성을 줄이는 효과가 있습니다.\n",
        "  <br> **bias (bool, optional)**: 편향(bias)을 사용할지 여부를 결정하는 플래그입니다. 기본값은 True이며, False로 설정하면 편향이 사용되지 않습니다.\n",
        "  <br> 우리 프로젝트 사용 예제\n",
        "  ~~~ python\n",
        "  class Mnist_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.view(-1, 1, 28, 28)\n",
        "        xb = F.relu(self.conv1(xb))\n",
        "        xb = F.relu(self.conv2(xb))\n",
        "        xb = F.relu(self.conv3(xb))\n",
        "        xb = F.avg_pool2d(xb, 4)\n",
        "        return xb.view(-1, xb.size(1))\n",
        "\n",
        "    lr = 0.1\n",
        "~~~\n",
        "\n",
        "  ~~~ python\n",
        "    model = Mnist_CNN()\n",
        "    opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
        "~~~\n",
        "  <br> **모멘텀**은 이전 업데이트도 고려하고 일반적으로 더 빠른 훈련으로 이어지는 확률적 경사하강법의 변형입니다.\n",
        "  ***\n",
        "   \n",
        "  7. **nn.Sequential 사용하기**\n",
        "  ***\n",
        "<br> nn.Sequential 은 순서를 갖는 모듈의 컨테이너입니다. 데이터는 정의된 것과 같은 순서로 모든 모듈들을 통해 전달됩니다. 순차 컨테이너(sequential container)를 사용하여 아래의 seq_modules 와 같은 신경망을 빠르게 만들 수 있습니다.\n",
        "***\n",
        "  \n",
        "  8. **파이토치 모듈을 사용해 본 느낌**\n",
        "  ***\n",
        "  <br> 처음 사용해 본 파이토치에서 어렵게만 느껴졌던 수학 공식에 근거한 신경망 구축이 파이토치의 깔끔한 모듈을 사용해보니 편리하고 코드도 짧아지고 매번 변수를 0으로 수동으로 초기화 해주는 어려움이나,\n",
        "  소프트맥스 같은 활성화 함수를 제공해주어 복잡한 계산에서는 변수를 잊어버리거나 잘못 사용하는 실수를 줄 일 수 있을것 같았다. 더군다나 좋았던 점은 파이토치에서 제공해주는 모듈을 사용하여 오차를 줄 일수 있다는 점이 매우 매력적 이였습니다.\n",
        "\n",
        "<br>\n",
        "***\n",
        "  9. **참고 문헌** :\n",
        "  [파이토치 한국 사용자 모임](https://tutorials.pytorch.kr/beginner/basics/buildmodel_tutorial.html) <br>\n",
        "  [위키독스](https://wikidocs.net/194918)"
      ],
      "metadata": {
        "id": "TL7DFqiddzB0"
      }
    }
  ]
}